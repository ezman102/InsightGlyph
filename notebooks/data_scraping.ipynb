{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Function to initialize Selenium WebDriver\n",
    "def init_driver(headless=True):\n",
    "    chrome_options = Options()\n",
    "    return webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Function to scroll the page by smaller increments and capture tweets at each step\n",
    "def scroll_and_scrape(driver, max_scrolls=20, scroll_pause_time=0.5):\n",
    "    tweets_data = []\n",
    "    seen_tweets = set()\n",
    "\n",
    "    def scrape_tweets_from_html(html):\n",
    "        nonlocal tweets_data, seen_tweets\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        tweet_divs = soup.find_all('article')\n",
    "\n",
    "        for tweet_div in tweet_divs:\n",
    "            tweet_text_tag = tweet_div.find(['div', 'span'], {'data-testid': 'tweetText'})\n",
    "            time_tag = tweet_div.find('time')\n",
    "\n",
    "            tweet_text = tweet_text_tag.get_text(separator=\" \", strip=True) if tweet_text_tag else 'No Text Found'\n",
    "            tweet_timestamp = time_tag['datetime'] if time_tag else 'No Timestamp Found'\n",
    "\n",
    "            if tweet_text and tweet_text not in seen_tweets:\n",
    "                tweets_data.append({\n",
    "                    \"text\": tweet_text,\n",
    "                    \"timestamp\": tweet_timestamp\n",
    "                })\n",
    "                seen_tweets.add(tweet_text)\n",
    "\n",
    "    scrape_tweets_from_html(driver.page_source)\n",
    "\n",
    "    # Scroll down the page in small increments and scrape on each scroll\n",
    "    for scroll_count in range(max_scrolls):\n",
    "        driver.execute_script(\"window.scrollBy(0, 300);\")  # Scroll by 300 pixels at a time\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scrape_tweets_from_html(driver.page_source)  # Scrape after each scroll\n",
    "        # print(f\"Scrolled {scroll_count + 1}/{max_scrolls} times.\")\n",
    "\n",
    "    # Scroll back up to capture any missed tweets\n",
    "    driver.execute_script(\"window.scrollTo(0, 0);\")\n",
    "    time.sleep(2)  # Wait a bit after scrolling up\n",
    "    scrape_tweets_from_html(driver.page_source)\n",
    "\n",
    "    return tweets_data\n",
    "\n",
    "# Function to scrape tweets from a given X.com URL\n",
    "def scrape_x_article(url, headless=True):\n",
    "    driver = init_driver(headless=headless)\n",
    "    driver.get(url)\n",
    "\n",
    "    try:\n",
    "        # Wait until at least one tweet is loaded\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.XPATH, \"//div[@data-testid='tweetText']\"))\n",
    "        )\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout while waiting for tweets to load on {url}\")\n",
    "        driver.quit()\n",
    "        return None\n",
    "\n",
    "    # Scroll and capture tweets at each scroll step\n",
    "    tweets_data = scroll_and_scrape(driver)\n",
    "\n",
    "    print(f\"Total unique tweets collected: {len(tweets_data)}\")\n",
    "\n",
    "    x_data = {\n",
    "        \"source\": \"X.com\",\n",
    "        \"url\": url,\n",
    "        \"tweets\": tweets_data\n",
    "    }\n",
    "\n",
    "    driver.quit()\n",
    "    return x_data\n",
    "\n",
    "# List of X.com URLs\n",
    "x_urls = [\"https://x.com/CNN\",\"https://x.com/TIME\",\"https://x.com/elonmusk\",\"https://x.com/BarackObama\",\"https://x.com/justinbieber\"]\n",
    "\n",
    "all_x_data = []\n",
    "for url in x_urls:\n",
    "    print(f\"Scraping URL: {url}\")\n",
    "    x_data = scrape_x_article(url, headless=False) \n",
    "    if x_data:\n",
    "        all_x_data.append(x_data)\n",
    "        \n",
    "for data in all_x_data:\n",
    "    print(json.dumps(data, indent=4, ensure_ascii=False))\n",
    "\n",
    "save_data_to_json(all_x_data, 'x_data.json')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
